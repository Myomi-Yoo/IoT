view_data <- url_data[(which(str_detect(url_data,"view_count"))+3)]
view_data
view_data <- url_data[(which(str_detect(url_data,"view_info"))+3)]
view_data <- url_data[(which(str_detect(url_data,"view_info")))]
view_data
view_data <- url_data[(which(str_detect(url_data,"view_info"))+6)]
view_data
view_data <- url_data[(which(str_detect(url_data,"view_info"))+5)]
view_data
content_data <- url_data[(which(str_detect(url_data,"post_article fr-view"))+1)]
content_data
content_data <- url_data[(which(str_detect(url_data,"post_article fr-view")))]
content_data
content_data <- url_data[which(str_detect(url_data,"post_content"))]
content_data
content_data <- str_extract(url_data, "(?<=class=\"post_article fr-view\">).*(?=</div>)")
content_data
content_data <- str_extract(url_data, "(?<=<p>).*(?=</p>)")
content_data <- url_data[(str_detect(url_data,"post_article"))]
content <- str_extract(url_data, "(?<=<p>).*(?=</p>)")
content
content_data <- url_data[(str_detect(url_data,"post_article"))]
content <- str_extract(content_data, "(?<=<p>).*(?=</p>)")
content
content_data <- url_data[which(str_detect(url_data,"post_article"))+1]
content_data
content_data <- url_data[which(str_detect(url_data,"post_article"))+2]
content_data
content_data <- url_data[which(str_detect(url_data,"post_article fr-view"))+2]
content_data
content_data <- url_data[which(str_detect(url_data,"post_article"))+2]
content_data
content_data <- url_data[which(str_detect(url_data,"post_article"))]
content_data
content_data <- url_data[which(str_detect(url_data,"post_article"))+4]
content_data
which(str_detect(url_data,"post_article"))
url_data[510]
url_data[518]
url_data[510]
content_data <- url_data[which(str_detect(url_data,"post_article"))+5]
content_data
content <- str_extract(content_data, "(?<=<p>).*(?=</p>)")
content
content_data <- url_data[which(str_detect(url_data,"post_article"))+5]
content_data
content <- str_extract(content_data, "(?<=<p>).*(?=</p>)")
content
which(str_detect(url_data,"/article"))
which(str_detect(url_data,"</article>"))
which(str_detect(url_data,"<article>"))
?for
?for
? for
library("stringr")
library("mongolite")
library("stringr")
for (url_data in which(str_detect(url_data,"<article>")):which(str_detect(url_data,"</article>"))) {
content <- str_extract(content_data, "(?<=<p>).*(?=</p>)")
content
}
for (url_data in which(str_detect(url_data,"<article>")):which(str_detect(url_data,"</article>"))) {
content <- str_extract(content_data, "(?<=<p>).*(?=</p>)")
}
for (url_data in url_data[which(str_detect(url_data,"<article>")):which(str_detect(url_data,"</article>"))]) {
content <- str_extract(content_data, "(?<=<p>).*(?=</p>)")
}
which(str_detect(url_data,"<article>"))
which(str_detect(url_data,"</article>"))
library("stringr")
library("mongolite")
load(file = "crawl_data.RData")
final_data
final_data[,3]
url <- final_data[2,3]
url_data <- readLines(url, encoding = "UTF-8")
url_data
title_data <- url_data[(which(str_detect(url_data,"post_subject"))+1)]
title_data
title <- str_extract(title_data, "(?<=<span>).*(?=</span>)")
title
which(str_detect(url_data,"<article>"))
which(str_detect(url_data,"</article>"))
content_data <- url_data[which(str_detect(url_data,"post_article"))+5]
content_data
content <- str_extract(content_data, "(?<=<p>).*(?=</p>)")
content
which(str_detect(url_data,"<article>"))
which(str_detect(url_data,"</article>"))
url_list <- final_data[,3]
class(url_list)
contentdata <- readLines(url_list[1], encoding = "UTF-8")
contentdata
contentdata <- readLines(url_list[1], encoding = "UTF-8")
class(url_list)
contentdata <- readLines(url_list[1], encoding = "UTF-8")
contentdata
class(contentdata)
start = str_detect(contentdata,"post_content")
start
start = which(str_detect(contentdata,"post_content"))
start
end = which(str_detect(contentdata,"post_ccls"))
end
contentdata <- readLines(url_list[2], encoding = "UTF-8")
contentdata
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
start
end
content_filter_data <- contentdata[start:end]
content_filter_data
#데이터 정제하기
#1. 벡터로 리턴하므로 한 개로 합치기
content_filter_data <- paste(content_filter_data,collapse = "")
content_filter_data
#2. 불필요한 기호나 태그 없애기
content_filter_data <- gsub("<.*>","",content_filter_data)
content_filter_data
#원하는 부분만 가지고 오기
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
start
end
content_filter_data <- contentdata[start:end]
content_filter_data
#데이터 정제하기
#1. 벡터로 리턴하므로 한 개로 합치기
content_filter_data <- paste(content_filter_data,collapse = "")
content_filter_data
#2. 불필요한 기호나 태그 없애기
#태그 없애기
content_filter_data <- gsub("<.*?>","",content_filter_data)
content_filter_data
content_filter_data <- gsub("\t|&nbsp;","",content_filter_data)
content_filter_data
url <-"https://www.clien.net/service/group/community"
url_data <- readLines(url,encoding = "UTF-8")
url_data
filter_data <- url_data[str_detect(url_data,"subject_fixed")]
title <- str_extract(filter_data,"(?<=\">).*(?=</span>)")
title
hit_data <- url_data[str_detect(url_data,"<span class=\"hit\">")]
hit <- str_extract(hit_data,"(?<=\">).*(?=</span>)")[-1]
t(t(hit))
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val <- paste0("http://www.clien.net",url_val)
url_val
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val,end = -3)
url_val <- paste0("http://www.clien.net",url_val)
url_val
contentdata <- readLines(url_val, encoding = "UTF-8")
contentdata <- readLines(url_val[1,], encoding = "UTF-8")
contentdata
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
start
end
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data,collapse = "")
content_filter_data <- gsub("<.*?>","",content_filter_data)
content_filter_data <- gsub("\t|&nbsp;","",content_filter_data)
content_filter_data
contentdata <- readLines(url_val, encoding = "UTF-8")
for (i in 1:30) {
contentdata <- readLines(url_val[i], encoding = "UTF-8")
contentdata
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
start
end
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data,collapse = "")
content_filter_data <- gsub("<.*?>","",content_filter_data)
content_filter_data <- gsub("\t|&nbsp;","",content_filter_data)
content_filter_data
}
content_filter_data
for (i in 1:30) {
contentdata <- readLines(url_val[i], encoding = "UTF-8")
contentdata
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
start
end
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data,collapse = "")
content_filter_data <- gsub("<.*?>","",content_filter_data)
content_filter_data <- gsub("\t|&nbsp;","",content_filter_data)
content <- cbind(content_filter_data)
}
content
final_data <- cbind(title,hit,url_val)
final_data
for (i in 1:30) {
contentdata <- readLines(url_val[i], encoding = "UTF-8")
contentdata
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
start
end
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data,collapse = "")
content_filter_data <- gsub("<.*?>","",content_filter_data)
content_filter_data <- gsub("\t|&nbsp;","",content_filter_data)
final_data$content <- content_filter_data
}
final_data
final_data
url <-"https://www.clien.net/service/group/community?&od=T31&po=0"
url_data <- readLines(url,encoding = "UTF-8")
url_data
filter_data <- url_data[str_detect(url_data,"subject_fixed")]
title <- str_extract(filter_data,"(?<=\">).*(?=</span>)")
title
hit_data <- url_data[str_detect(url_data,"<span class=\"hit\">")]
hit <- str_extract(hit_data,"(?<=\">).*(?=</span>)")[-1]
t(t(hit))
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val,end = -3)
url_val <- paste0("http://www.clien.net",url_val)
url_val
final_data <- cbind(title,hit,url_val)
final_data
mym
#1부터 15까지의 값을 3개의 열로 표현, row방향 값을 추가
mymat1 <-  matrix(1:15,ncol=3,byrow=T)
addmymat2 <- rbind(mymat1,c(1,11,111,1111))
addmymat2
library("stringr")
library("mongolite")
con <- mongo(collection = "crawl",
db = "bigdata",
url = "mongodb://127.0.0.1")
#0번 부터 9번 페이지까지 반복 작업
final_data_list = NULL
for (i in 0:9) {
#페이지마다 연결할 주소가 달라지므로 변수로 처리
url <- paste0("https://www.clien.net/service/group/community?&od=T31&po=",i)
url_data <- readLines(url,encoding = "UTF-8")
url_data
##title 추출
final_filter_data <- url_data[str_detect(url_data,"subject_fixed")]
title <- str_extract(final_filter_data,"(?<=\">).*(?=</span>)")
title
##hit 추출
hit_data <- url_data[str_detect(url_data,"<span class=\"hit\">")]
hit <- str_extract(hit_data,"(?<=\">).*(?=</span>)")[-1]
t(t(hit))
##url 추출
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val,end = -3)
url_val <- paste0("http://www.clien.net",url_val)
url_val
####### url을 이용해서 content항목 추출
contentlist = NULL #최초 변수 선언시 null로 초기화
for (page in 1:length(url_val)) {
contentdata <- readLines(url_val[page], encoding = "UTF-8")
contentdata
start = which(str_detect(contentdata,"post_content"))
end = which(str_detect(contentdata,"post_ccls"))
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data,collapse = "")
content_filter_data <- gsub("<.*?>","",content_filter_data)
content_filter_data <- gsub("\t|&nbsp;","",content_filter_data)
#기존에 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist,content_filter_data)
cat("\n",page)
}
final_data <- cbind(title,hit,url_val,contentlist)
final_data_list <- rbind(final_data_list,final_data)
cat("\n",i)
}
final_data
write.csv(final_data_list,"final_data.csv")
save(final_data_list,file = "final_data.RData")
####mongodb에 저장하기####
if(con$count()>0){
con$drop()
}
##mongodb에 데이터를 저장하기 위해서 dataframe으로 변환
final_mongo_data <- data.frame(final_data_list)
con$insert(final_data_list)
install.packages("N2H4")
library(N2H4)
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
comments <- getAllComment(url)
comments
library(stringr)
library(dplyr)
getAllComment(url) %>%
select(username, content) -> conmments
getAllComment(url) %>%
select(userName, content) -> conmments
comments
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
getAllComment(url) %>%
select(userName, content)
getAllComment(url) %>%
select(userName, content) -> mydata
mydata
library(N2H4)
library(stringr)
library(dplyr)
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
getAllComment(url) %>%
select(userName, content) -> mydata
mydata
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
getAllComment(url) %>%
select(userName, contents) -> mydata
mydata
class(mydata)
mycomment <- mydata$contents
mycomment
#css의 선택지를 활용할 수 있는 방법 - rvest
install.packages("rvest")
install.packages("rvest")
library(rvest)
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
readPage <- read_html(url)
readPage %>%
html_nodes("span.subject_fixed") -> title_data
title_data
readPage <- read_html(url)
readPage %>%
html_nodes("span.subject_fixed") %>%
html_text() -> title_data
title_data
install.packages("KoNLP")
install.packages("KoNLP")
library(KoNLP)
install.packages("Sejong")
library(KoNLP)
install.packages("hash")
library(KoNLP)
install.packages("rJava")
library(KoNLP)
install.packages("tau")
library(KoNLP)
install.packages("RSQLite")
library(KoNLP)
install.packages("devtools")
library(KoNLP)
library(stringr)
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
#######분석할 샘플데이터 로딩
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
#######형태소분석을 하기 위해서 명사분리#######
data_list <- list()
#######형태소분석을 하기 위해서 명사분리#######
#댓글을 분리하면 분리된 명사의 갯수가 다르므로 리스트를 이용
data_list <- list()
for (i in 1:length(sampledata)) {
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[2]]
data_list[[22]]
head(data_list,20)
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#                 => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,3,3,3,3,3)
), #반복작업할 데이터
function(x){
x[1]
} # 반복해서 적용할 함수
)
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#                 => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,4,5,6,7,8)
), #반복작업할 데이터
function(x){
x[1]
} # 반복해서 적용할 함수
)
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#                 => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), #반복작업할 데이터
#        function(x){
#          x[1]
#        } # 반복해서 적용할 함수
#        )
class(data_list)
class(wordlist)
library(KoNLP)
library(stringr)
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
#######형태소분석을 하기 위해서 명사분리#######
#댓글을 분리하면 분리된 명사의 갯수가 다르므로 리스트를 이용
data_list <- list()
for (i in 1:length(sampledata)) {
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
head(data_list,20)
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#                 => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), #반복작업할 데이터
#        function(x){
#          x[1]
#        } # 반복해서 적용할 함수
#        )
class(data_list)
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
class(wordlist)
wordlist
#리스트를
class(unlist(data_list))
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
head(wordlist, 20)
head(data_list, 20)
wordlist <- sapply(str_split(unlist(data_list),"/"), function(x){
x[1]
})
head(wordlist, 20)
head(data_list, 20)
tablewordlist[1]
#table함수를 이용해서 단어의 빈도수를 구하기
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,decreasing = T)[1:100]
#분석결과를 가지고 기준을 적용해서 정리 - 한 글자를 빼고
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist,decreasing = T)[1:100]
tablewordlist_result
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist,decreasing = T)[1:100]
tablewordlist_result
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T)[1:100]
tablewordlist_result
install.packages("wordcloud")
install.packages("Rcolor")
install.packages("RColorBrewer")
library(wordcloud)
library(RColorBrewer)
#RcolorBrewer
display.brewer.all(n = 10,exact.n = F)
? palette.info
brewer.pal.info
display.brewer.all(type = "div")
display.brewer.all(type = "qual")
display.brewer.all(type = "seq")
#분석한 결과가 저장되어 있는 tablewordlist_result의 값을 단어와 숫자를 각각 저장
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
mycolor <- brewer.pal(n=11,name="RdYlGn")
wordcloud(words = word, freq = count, random.order = F, colors = mycolor,
scale = c(7,0.3))
brewer.pal.info
display.brewer.all(type = "div")
display.brewer.all(type = "qual")
mycolor <- brewer.pal(n=11,name="Pastel2")
wordcloud(words = word, freq = count, random.order = F, colors = mycolor,
scale = c(7,0.3))
display.brewer.all(type = "div")
display.brewer.all(type = "seq")
mycolor <- brewer.pal(n=11,name="PuBu")
wordcloud(words = word, freq = count, random.order = F, colors = mycolor,
scale = c(7,0.3))
mycolor <- brewer.pal(n=11,name="Set1")
wordcloud(words = word, freq = count, random.order = F, colors = mycolor,
scale = c(7,0.3))
mycolor <- brewer.pal(n=9,name="Set1")
wordcloud(words = word, freq = count, random.order = F, colors = mycolor,
scale = c(7,0.3))
tablewordlist_result
